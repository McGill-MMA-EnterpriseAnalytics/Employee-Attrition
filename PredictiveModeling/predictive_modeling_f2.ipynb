{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "from sklearn.metrics import fbeta_score, make_scorer, roc_auc_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
    "df.drop(columns=['EmployeeCount', 'StandardHours', 'EmployeeNumber'], inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Convert Attrition to binary: 'Yes' → 1, 'No' → 0.\n",
    "df[\"Attrition\"] = df[\"Attrition\"].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "X = df.drop(\"Attrition\", axis=1)\n",
    "y = df[\"Attrition\"]\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "# Reset indices to ensure alignment\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test  = X_test.reset_index(drop=True)\n",
    "y_test  = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <th>Gender</th>\n",
       "      <th>HourlyRate</th>\n",
       "      <th>...</th>\n",
       "      <th>PerformanceRating</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1225</td>\n",
       "      <td>Sales</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>594</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Technical Degree</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>406</td>\n",
       "      <td>Sales</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>622</td>\n",
       "      <td>Sales</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1001</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>Medical</td>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>23</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>427</td>\n",
       "      <td>Sales</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>38</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1009</td>\n",
       "      <td>Sales</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>22</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>217</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>36</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>430</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Other</td>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>39</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>766</td>\n",
       "      <td>Sales</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1176 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age     BusinessTravel  DailyRate              Department  \\\n",
       "0      47      Travel_Rarely       1225                   Sales   \n",
       "1      22      Travel_Rarely        594  Research & Development   \n",
       "2      46      Travel_Rarely        406                   Sales   \n",
       "3      25      Travel_Rarely        622                   Sales   \n",
       "4      43  Travel_Frequently       1001  Research & Development   \n",
       "...   ...                ...        ...                     ...   \n",
       "1171   23      Travel_Rarely        427                   Sales   \n",
       "1172   38      Travel_Rarely       1009                   Sales   \n",
       "1173   22      Travel_Rarely        217  Research & Development   \n",
       "1174   36      Travel_Rarely        430  Research & Development   \n",
       "1175   39  Travel_Frequently        766                   Sales   \n",
       "\n",
       "      DistanceFromHome  Education    EducationField  EnvironmentSatisfaction  \\\n",
       "0                    2          4     Life Sciences                        2   \n",
       "1                    2          1  Technical Degree                        3   \n",
       "2                    3          1         Marketing                        1   \n",
       "3                   13          1           Medical                        2   \n",
       "4                    9          5           Medical                        4   \n",
       "...                ...        ...               ...                      ...   \n",
       "1171                 7          3     Life Sciences                        3   \n",
       "1172                 2          2     Life Sciences                        2   \n",
       "1173                 8          1     Life Sciences                        2   \n",
       "1174                 2          4             Other                        4   \n",
       "1175                20          3     Life Sciences                        3   \n",
       "\n",
       "      Gender  HourlyRate  ...  PerformanceRating  RelationshipSatisfaction  \\\n",
       "0     Female          47  ...                  3                         3   \n",
       "1       Male         100  ...                  3                         3   \n",
       "2       Male          52  ...                  3                         4   \n",
       "3       Male          40  ...                  3                         3   \n",
       "4       Male          72  ...                  3                         2   \n",
       "...      ...         ...  ...                ...                       ...   \n",
       "1171    Male          99  ...                  4                         2   \n",
       "1172  Female          31  ...                  3                         4   \n",
       "1173    Male          94  ...                  3                         1   \n",
       "1174  Female          73  ...                  4                         4   \n",
       "1175    Male          83  ...                  3                         4   \n",
       "\n",
       "     StockOptionLevel  TotalWorkingYears TrainingTimesLastYear  \\\n",
       "0                   3                 29                     2   \n",
       "1                   1                  3                     2   \n",
       "2                   1                 23                     3   \n",
       "3                   0                  7                     1   \n",
       "4                   1                 10                     3   \n",
       "...               ...                ...                   ...   \n",
       "1171                1                  3                     2   \n",
       "1172                1                 11                     3   \n",
       "1173                1                  4                     3   \n",
       "1174                1                 15                     2   \n",
       "1175                1                  7                     6   \n",
       "\n",
       "      WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n",
       "0                   3               3                   2   \n",
       "1                   3               2                   1   \n",
       "2                   3              12                   9   \n",
       "3                   3               7                   4   \n",
       "4                   3               8                   7   \n",
       "...               ...             ...                 ...   \n",
       "1171                3               3                   2   \n",
       "1172                3               7                   7   \n",
       "1173                2               4                   3   \n",
       "1174                3               1                   0   \n",
       "1175                3               2                   1   \n",
       "\n",
       "     YearsSinceLastPromotion YearsWithCurrManager  \n",
       "0                          1                    2  \n",
       "1                          2                    1  \n",
       "2                          4                    9  \n",
       "3                          0                    6  \n",
       "4                          4                    7  \n",
       "...                      ...                  ...  \n",
       "1171                       0                    2  \n",
       "1172                       1                    7  \n",
       "1173                       1                    1  \n",
       "1174                       0                    0  \n",
       "1175                       2                    2  \n",
       "\n",
       "[1176 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Columns to keep as ordinal (already integers)\n",
    "ordinal_cols = ['Education', 'EnvironmentSatisfaction', 'JobInvolvement',\n",
    "                'JobLevel', 'JobSatisfaction', 'PerformanceRating',\n",
    "                'RelationshipSatisfaction', 'StockOptionLevel', 'WorkLifeBalance']\n",
    "\n",
    "# For numeric columns, we want to apply log transform only on those that are not ordinal.\n",
    "numeric_to_transform = [col for col in numeric_cols if col not in ordinal_cols]\n",
    "\n",
    "# For categorical columns, separate BusinessTravel so we can experiment with its encoding.\n",
    "business_travel = ['BusinessTravel']\n",
    "other_cat_cols = [col for col in categorical_cols if col not in business_travel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNewFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds three new features:\n",
    "      - AgeAtJoining = Age - YearsAtCompany\n",
    "      - TenureRatio = YearsAtCompany / TotalWorkingYears\n",
    "      - IncomePerYearExp = MonthlyIncome / TotalWorkingYears\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        # Make sure these columns exist\n",
    "        if 'Age' in X_.columns and 'YearsAtCompany' in X_.columns:\n",
    "            X_['AgeAtJoining'] = X_['Age'] - X_['YearsAtCompany']\n",
    "        if 'YearsAtCompany' in X_.columns and 'TotalWorkingYears' in X_.columns:\n",
    "            # Avoid division by zero\n",
    "            ratio = X_['YearsAtCompany'] / X_['TotalWorkingYears'].replace(0, np.nan)\n",
    "            X_['TenureRatio'] = ratio.fillna(0)\n",
    "        if 'MonthlyIncome' in X_.columns and 'TotalWorkingYears' in X_.columns:\n",
    "            ratio2 = X_['MonthlyIncome'] / X_['TotalWorkingYears'].replace(0, np.nan)\n",
    "            X_['IncomePerYearExp'] = ratio2.fillna(0)\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class CustomOrdinalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mapping=None):\n",
    "        # Default mapping if none is provided.\n",
    "        if mapping is None:\n",
    "            mapping = {'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2}\n",
    "        self.mapping = mapping\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Optionally, you could store the unique categories here.\n",
    "        # For now, simply mark the encoder as fitted.\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure the transformer has been fitted.\n",
    "        if not hasattr(self, 'fitted_'):\n",
    "            raise ValueError(\"This CustomOrdinalEncoder instance is not fitted yet. Call 'fit' before using this method.\")\n",
    "        \n",
    "        # If X is a numpy array and has shape (n_samples, 1), flatten it.\n",
    "        if isinstance(X, np.ndarray):\n",
    "            if X.ndim == 2 and X.shape[1] == 1:\n",
    "                s = pd.Series(X.ravel())\n",
    "                mapped = s.map(self.mapping)\n",
    "                if mapped.isnull().any():\n",
    "                    raise ValueError(\"Some values were not found in the mapping\")\n",
    "                return mapped.values.reshape(-1, 1)\n",
    "            else:\n",
    "                raise ValueError(\"Expected a 2D array with a single column\")\n",
    "        else:\n",
    "            # If X is a pandas DataFrame or Series.\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                if X.shape[1] == 1:\n",
    "                    s = X.iloc[:, 0]\n",
    "                    mapped = s.map(self.mapping)\n",
    "                    if mapped.isnull().any():\n",
    "                        raise ValueError(\"Some values were not found in the mapping\")\n",
    "                    return mapped.values.reshape(-1, 1)\n",
    "                else:\n",
    "                    # If there are multiple columns, apply mapping on each.\n",
    "                    return X.apply(lambda col: col.map(self.mapping)).values\n",
    "            elif isinstance(X, pd.Series):\n",
    "                mapped = X.map(self.mapping)\n",
    "                if mapped.isnull().any():\n",
    "                    raise ValueError(\"Some values were not found in the mapping\")\n",
    "                return mapped.values.reshape(-1, 1)\n",
    "            else:\n",
    "                raise ValueError(\"Input type not recognized. Expected numpy array or pandas DataFrame/Series.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.base import BaseSampler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class IsolationForestRemover(BaseSampler):\n",
    "    \"\"\"\n",
    "    An outlier removal step using IsolationForest, implemented as an imblearn 'sampler'.\n",
    "    This approach aligns with how ImbPipeline expects sampling methods to behave.\n",
    "    \"\"\"\n",
    "    _parameter_constraints = {}\n",
    "    _sampling_type = 'clean-sampling'\n",
    "\n",
    "    def __init__(self, contamination=0.05, random_state=42):\n",
    "        super().__init__()\n",
    "        self.contamination = contamination\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        \"\"\"\n",
    "        The core method required by 'BaseSampler' subclasses.\n",
    "        It fits the IsolationForest on X, then filters out rows (and corresponding y)\n",
    "        flagged as outliers.\n",
    "        \"\"\"\n",
    "        iso = IsolationForest(\n",
    "            contamination=self.contamination,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        iso.fit(X)\n",
    "        preds = iso.predict(X)\n",
    "        mask = (preds == 1)  # Keep only inliers\n",
    "        return X[mask], y[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "class BoxCoxSkewedTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Applies Box-Cox transformation only to the columns specified in skewed_cols.\n",
    "    Automatically shifts data if min value <= 0.\n",
    "    \"\"\"\n",
    "    def __init__(self, skewed_cols=None):\n",
    "        if skewed_cols is None:\n",
    "            skewed_cols = []\n",
    "        self.skewed_cols = skewed_cols\n",
    "        self.col_names_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Capture column names if X is a DataFrame; otherwise, use generic names.\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.col_names_ = X.columns.tolist()\n",
    "        else:\n",
    "            self.col_names_ = [f\"col_{i}\" for i in range(X.shape[1])]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convert to DataFrame using stored column names\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X_df = pd.DataFrame(X, columns=self.col_names_)\n",
    "        else:\n",
    "            X_df = X.copy()\n",
    "\n",
    "        # Apply Box-Cox only to self.skewed_cols (if they exist in X_df)\n",
    "        for col in self.skewed_cols:\n",
    "            if col in X_df.columns:\n",
    "                col_min = X_df[col].min()\n",
    "                shift = 0\n",
    "                if col_min <= 0:\n",
    "                    shift = abs(col_min) + 1\n",
    "                # boxcox returns a tuple (transformed, lambda), so we take index [0]\n",
    "                X_df[col] = boxcox(X_df[col] + shift)[0]\n",
    "\n",
    "        # Return values as a NumPy array to remain consistent with scikit-learn\n",
    "        return X_df.values\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"\n",
    "        If input_features is provided, return it (typical in ColumnTransformer),\n",
    "        otherwise return the stored col_names_.\n",
    "        \"\"\"\n",
    "        if input_features is not None:\n",
    "            return np.array(input_features)\n",
    "        return np.array(self.col_names_ if self.col_names_ is not None else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LogTransformSkewed(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, skewed_cols=None):\n",
    "        if skewed_cols is None:\n",
    "            skewed_cols = []\n",
    "        self.skewed_cols = skewed_cols\n",
    "        self.col_names_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # If X is a DataFrame, store its columns. Otherwise, just store generic names.\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.col_names_ = list(X.columns)\n",
    "        else:\n",
    "            self.col_names_ = [f\"col_{i}\" for i in range(X.shape[1])]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convert NumPy array to DataFrame using stored col names (if needed)\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X_df = pd.DataFrame(X, columns=self.col_names_)\n",
    "        else:\n",
    "            X_df = X.copy()  # Already a DataFrame\n",
    "\n",
    "        # Apply log transform to only the specified skewed columns\n",
    "        for col in self.skewed_cols:\n",
    "            if col in X_df.columns:\n",
    "                X_df[col] = np.log1p(X_df[col])\n",
    "\n",
    "        return X_df.values  # Return as array for downstream steps\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        # So the pipeline can retrieve feature names if needed\n",
    "        if input_features is not None:\n",
    "            return np.array(input_features)\n",
    "        return np.array(self.col_names_ if self.col_names_ is not None else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monkey_patch_get_signature_names_out():\n",
    "    from inspect import Signature, signature, Parameter\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "    import pandas as pd\n",
    "\n",
    "    default_get_feature_names_out = StandardScaler.get_feature_names_out\n",
    "\n",
    "    if not hasattr(SimpleImputer, \"get_feature_names_out\"):\n",
    "        SimpleImputer.get_feature_names_out = default_get_feature_names_out\n",
    "\n",
    "    if not hasattr(FunctionTransformer, \"get_feature_names_out\"):\n",
    "        orig_init = FunctionTransformer.__init__\n",
    "        orig_sig = signature(orig_init)\n",
    "\n",
    "        def __init__(*args, feature_names_out=None, **kwargs):\n",
    "            orig_sig.bind(*args, **kwargs)\n",
    "            orig_init(*args, **kwargs)\n",
    "            args[0].feature_names_out = feature_names_out\n",
    "\n",
    "        __init__.__signature__ = Signature(\n",
    "            list(signature(orig_init).parameters.values()) + [\n",
    "                Parameter(\"feature_names_out\", Parameter.KEYWORD_ONLY)]\n",
    "        )\n",
    "\n",
    "        def get_feature_names_out(self, names=None):\n",
    "            if callable(self.feature_names_out):\n",
    "                return self.feature_names_out(self, names)\n",
    "            assert self.feature_names_out == \"one-to-one\"\n",
    "            return default_get_feature_names_out(self, names)\n",
    "\n",
    "        FunctionTransformer.__init__ = __init__\n",
    "        FunctionTransformer.get_feature_names_out = get_feature_names_out\n",
    "\n",
    "    if not hasattr(CustomOrdinalEncoder, \"get_feature_names_out\"):\n",
    "        def custom_ordinal_get_feature_names_out(self, input_features=None):\n",
    "            if input_features is not None and len(input_features) > 0:\n",
    "                return np.array(input_features)\n",
    "            else:\n",
    "                return np.array([\"custom_ordinal_encoded_feature\"])\n",
    "        CustomOrdinalEncoder.get_feature_names_out = custom_ordinal_get_feature_names_out\n",
    "\n",
    "    if not hasattr(AddNewFeaturesTransformer, \"get_feature_names_out\"):\n",
    "        def add_new_feats_get_feature_names_out(self, input_features=None):\n",
    "            \"\"\"\n",
    "            After transformation, new columns are added. This is a simplistic approach \n",
    "            that just appends the new column names.\n",
    "            \"\"\"\n",
    "            if input_features is None:\n",
    "                return np.array([\"Age\", \"BusinessTravel\", \"DailyRate\", \"Department\", \"DistanceFromHome\",\n",
    "                                 \"Education\", \"EducationField\", \"EnvironmentSatisfaction\", \"Gender\",\n",
    "                                 \"HourlyRate\", \"JobInvolvement\", \"JobLevel\", \"JobRole\", \"JobSatisfaction\",\n",
    "                                 \"MaritalStatus\", \"MonthlyIncome\", \"MonthlyRate\", \"NumCompaniesWorked\",\n",
    "                                 \"Over18\", \"OverTime\", \"PercentSalaryHike\", \"PerformanceRating\",\n",
    "                                 \"RelationshipSatisfaction\", \"StockOptionLevel\", \"TotalWorkingYears\",\n",
    "                                 \"TrainingTimesLastYear\", \"WorkLifeBalance\", \"YearsAtCompany\",\n",
    "                                 \"YearsInCurrentRole\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\",\n",
    "                                 # The new columns:\n",
    "                                 \"AgeAtJoining\", \"TenureRatio\", \"IncomePerYearExp\"])\n",
    "            else:\n",
    "                # If we started with known features, append the new feature names:\n",
    "                return np.concatenate([input_features, \n",
    "                                       np.array([\"AgeAtJoining\", \"TenureRatio\", \"IncomePerYearExp\"])])\n",
    "        AddNewFeaturesTransformer.get_feature_names_out = add_new_feats_get_feature_names_out\n",
    "\n",
    "    if not hasattr(IsolationForestRemover, \"get_feature_names_out\"):\n",
    "        def iso_remover_get_feature_names_out(self, input_features=None):\n",
    "            if input_features is not None:\n",
    "                return np.array(input_features)\n",
    "            else:\n",
    "                return np.array([\"filtered_features\"])\n",
    "        IsolationForestRemover.get_feature_names_out = iso_remover_get_feature_names_out\n",
    "\n",
    "monkey_patch_get_signature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. Define Helper Functions\n",
    "# =========================\n",
    "\n",
    "\n",
    "skewed_cols = [\n",
    "    'IncomePerYearExp', 'YearsSinceLastPromotion', 'YearsAtCompany', \n",
    "    'MonthlyIncome', 'TotalWorkingYears', 'NumCompaniesWorked', 'DistanceFromHome', \n",
    "    'YearsInCurrentRole', 'PercentSalaryHike', 'YearsWithCurrManager'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "log_transformer = LogTransformSkewed(skewed_cols=skewed_cols)\n",
    "boxcox_transformer = BoxCoxSkewedTransformer(skewed_cols=skewed_cols)\n",
    "no_transformer = 'passthrough'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline step to optionally remove outliers (IsolationForest) or do nothing.\n",
    "outlier_step = [\n",
    "    ('iso_remover', IsolationForestRemover()),  # or 'passthrough'\n",
    "]\n",
    "\n",
    "# Add new features\n",
    "add_features_step = [\n",
    "    ('add_new_feats', AddNewFeaturesTransformer())\n",
    "]\n",
    "\n",
    "# Numeric pipeline: median imputation -> log/boxcox/no transform -> scale (Standard or MinMax)\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('transformer', log_transformer),  # Will param-grid over log_transformer, boxcox_transformer, 'passthrough'\n",
    "    ('scaler', StandardScaler())       # Will param-grid over StandardScaler or MinMaxScaler\n",
    "])\n",
    "\n",
    "# Ordinal columns: pass them as-is\n",
    "ordinal_pipeline = 'passthrough'\n",
    "\n",
    "# BusinessTravel: OneHot or CustomOrdinal\n",
    "bus_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first'))  # We'll param-grid over OneHot or CustomOrdinal\n",
    "])\n",
    "\n",
    "# 4f. Other categorical columns: OneHot\n",
    "other_cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "# 4g. Build the overall preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, numeric_to_transform),\n",
    "    ('ord', ordinal_pipeline, ordinal_cols),\n",
    "    ('bus', bus_pipeline, business_travel),\n",
    "    ('other_cat', other_cat_pipeline, other_cat_cols)\n",
    "])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feat_sel_rfe = RFE(estimator=LogisticRegression(max_iter=500, random_state=42), n_features_to_select=10)\n",
    "feat_sel_lasso = SelectFromModel(LogisticRegression(penalty='l1', solver='liblinear', random_state=42))\n",
    "feat_sel_tree = SelectFromModel(RandomForestClassifier(random_state=42), threshold='median')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create an F2 scorer: beta=2 gives more weight to recall.\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# Define the base models with fixed (default) hyperparameters.\n",
    "base_models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=500, random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'MLP': MLPClassifier(max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "# Our full pipeline: optional outlier removal -> add new features -> preprocessor -> SMOTE -> feature selection -> classifier\n",
    "full_pipeline = ImbPipeline([\n",
    "    ('add_feats', AddNewFeaturesTransformer()),\n",
    "    ('preprocessor', clone(preprocessor)),\n",
    "    ('outlier', 'passthrough'),        # param-grid over IsolationForestRemover or pass\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('feat_sel', 'passthrough'),\n",
    "    ('classifier', LogisticRegression())  # placeholder, replaced per model\n",
    "])\n",
    "\n",
    "# Param grid for pipeline steps (not the classifier's internal hyperparams yet)\n",
    "pipeline_param_grid = {\n",
    "    'outlier': [IsolationForestRemover(contamination=0.05, random_state=42), 'passthrough'],\n",
    "    'preprocessor__num__transformer': [log_transformer, boxcox_transformer, no_transformer],\n",
    "    'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
    "    'preprocessor__bus__encoder': [OneHotEncoder(drop='first'), CustomOrdinalEncoder()],\n",
    "    'feat_sel': ['passthrough', feat_sel_rfe, feat_sel_lasso, feat_sel_tree]\n",
    "}\n",
    "\n",
    "# Dictionary to store the best pipeline for each base model (without classifier hyperparameter tuning)\n",
    "best_pipeline_nonhyper = {}\n",
    "\n",
    "best_pipeline_nonhyper = {}\n",
    "for model_name, model in base_models.items():\n",
    "    print(f\"\\nTuning pipeline steps for {model_name}...\")\n",
    "    \n",
    "    # Clone the full pipeline and replace the classifier\n",
    "    pipeline_model = clone(full_pipeline)\n",
    "    pipeline_model.set_params(classifier=model)\n",
    "    \n",
    "    grid_search_model = GridSearchCV(\n",
    "        pipeline_model,\n",
    "        param_grid=pipeline_param_grid,\n",
    "        cv=5,\n",
    "        scoring=f2_scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    grid_search_model.fit(X_train, y_train)\n",
    "    \n",
    "    best_pipeline = grid_search_model.best_estimator_\n",
    "    best_pipeline_nonhyper[model_name] = best_pipeline\n",
    "    print(f\"Best pipeline for {model_name}: {grid_search_model.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression:\n",
      "  Mean CV F2 Score     = 0.6226 (Std: 0.0817)\n",
      "  Mean CV Recall       = 0.7368 (Std: 0.1012)\n",
      "  Mean CV Precision    = 0.3868 (Std: 0.0550)\n",
      "  Mean CV ROC-AUC      = 0.8279 (Std: 0.0542)\n",
      "  Mean CV Accuracy     = 0.7671 (Std: 0.0373)\n",
      "----------------------------------------\n",
      "DecisionTree:\n",
      "  Mean CV F2 Score     = 0.3947 (Std: 0.0574)\n",
      "  Mean CV Recall       = 0.4211 (Std: 0.0725)\n",
      "  Mean CV Precision    = 0.3253 (Std: 0.0413)\n",
      "  Mean CV ROC-AUC      = 0.6271 (Std: 0.0322)\n",
      "  Mean CV Accuracy     = 0.7627 (Std: 0.0276)\n",
      "----------------------------------------\n",
      "RandomForest:\n",
      "  Mean CV F2 Score     = 0.3839 (Std: 0.0612)\n",
      "  Mean CV Recall       = 0.3684 (Std: 0.0725)\n",
      "  Mean CV Precision    = 0.4849 (Std: 0.0594)\n",
      "  Mean CV ROC-AUC      = 0.7281 (Std: 0.0280)\n",
      "  Mean CV Accuracy     = 0.8325 (Std: 0.0138)\n",
      "----------------------------------------\n",
      "GradientBoosting:\n",
      "  Mean CV F2 Score     = 0.4607 (Std: 0.0682)\n",
      "  Mean CV Recall       = 0.4316 (Std: 0.0636)\n",
      "  Mean CV Precision    = 0.6321 (Std: 0.0987)\n",
      "  Mean CV ROC-AUC      = 0.7958 (Std: 0.0617)\n",
      "  Mean CV Accuracy     = 0.8674 (Std: 0.0212)\n",
      "----------------------------------------\n",
      "MLP:\n",
      "  Mean CV F2 Score     = 0.4994 (Std: 0.0994)\n",
      "  Mean CV Recall       = 0.5000 (Std: 0.1079)\n",
      "  Mean CV Precision    = 0.5015 (Std: 0.0755)\n",
      "  Mean CV ROC-AUC      = 0.8008 (Std: 0.0494)\n",
      "  Mean CV Accuracy     = 0.8402 (Std: 0.0223)\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/Documents/Employee-Attrition/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richard/Documents/Employee-Attrition/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richard/Documents/Employee-Attrition/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richard/Documents/Employee-Attrition/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richard/Documents/Employee-Attrition/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def display_cv_f2_recall_precision_roc(pipeline_dict, X, y, cv=5):\n",
    "    scoring = {\n",
    "        'f2': f2_scorer,\n",
    "        'recall': 'recall',\n",
    "        'precision': 'precision',\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'accuracy': 'accuracy' \n",
    "    }\n",
    "    \n",
    "    for model_name, pipeline in pipeline_dict.items():\n",
    "        scores = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "        \n",
    "        mean_f2 = scores['test_f2'].mean()\n",
    "        std_f2 = scores['test_f2'].std()\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        std_recall = scores['test_recall'].std()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "        std_precision = scores['test_precision'].std()\n",
    "        mean_roc_auc = scores['test_roc_auc'].mean()\n",
    "        std_roc_auc = scores['test_roc_auc'].std()\n",
    "        mean_accuracy = scores['test_accuracy'].mean()  # Access accuracy results\n",
    "        std_accuracy = scores['test_accuracy'].std()\n",
    "        \n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  Mean CV F2 Score     = {mean_f2:.4f} (Std: {std_f2:.4f})\")\n",
    "        print(f\"  Mean CV Recall       = {mean_recall:.4f} (Std: {std_recall:.4f})\")\n",
    "        print(f\"  Mean CV Precision    = {mean_precision:.4f} (Std: {std_precision:.4f})\")\n",
    "        print(f\"  Mean CV ROC-AUC      = {mean_roc_auc:.4f} (Std: {std_roc_auc:.4f})\")\n",
    "        print(f\"  Mean CV Accuracy     = {mean_accuracy:.4f} (Std: {std_accuracy:.4f})\")\n",
    "        print(\"-\" * 40)\n",
    "display_cv_f2_recall_precision_roc(best_pipeline_nonhyper, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grids for each classifier\n",
    "classifier_grids = {\n",
    "    'LogisticRegression': {\n",
    "         'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "         'classifier__penalty': [None, 'l1', 'l2'],\n",
    "         'classifier__solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "         'classifier__max_depth': [None, 5, 10, 20],\n",
    "         'classifier__min_samples_split': [2, 5, 10],\n",
    "         'classifier__min_samples_leaf': [1, 2, 4],\n",
    "         'classifier__criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "         'classifier__n_estimators': [100, 200, 300],\n",
    "         'classifier__max_depth': [None, 5, 10, 20],\n",
    "         'classifier__min_samples_split': [2, 5, 10],\n",
    "         'classifier__min_samples_leaf': [1, 2, 4],\n",
    "         'classifier__max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "         'classifier__n_estimators': [100, 200, 300],\n",
    "         'classifier__learning_rate': [0.01, 0.1, 1],\n",
    "         'classifier__max_depth': [3, 5, 7, 10],\n",
    "         'classifier__subsample': [0.8, 1.0],\n",
    "         'classifier__max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    'MLP': {\n",
    "         'classifier__hidden_layer_sizes': [(50,), (100,), (50,50), (100,50)],\n",
    "         'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "         'classifier__learning_rate_init': [0.001, 0.01],\n",
    "         'classifier__activation': ['relu', 'tanh']\n",
    "    }\n",
    "}\n",
    "\n",
    "best_classifier_pipelines_random = {}\n",
    "n_iter = 50  # adjust as needed\n",
    "\n",
    "for clf_name, clf_grid in classifier_grids.items():\n",
    "    print(f\"\\nTuning classifier hyperparameters with random search: {clf_name}\")\n",
    "    \n",
    "    # Create new pipeline using the best pipeline steps found for this classifier\n",
    "    best_pipe_steps = best_pipeline_nonhyper[clf_name]\n",
    "    \n",
    "    # Build a new pipeline using those steps, but we replace the classifier hyperparams\n",
    "    tuned_pipeline = ImbPipeline([\n",
    "        ('add_feats', best_pipe_steps.named_steps['add_feats']),\n",
    "        ('preprocessor', best_pipe_steps.named_steps['preprocessor']),\n",
    "        ('outlier', best_pipe_steps.named_steps['outlier']),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('feat_sel', best_pipe_steps.named_steps['feat_sel']),\n",
    "        ('classifier', clone(base_models[clf_name]))\n",
    "    ])\n",
    "    \n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "         tuned_pipeline,\n",
    "         param_distributions=clf_grid,\n",
    "         n_iter=n_iter,\n",
    "         cv=5,\n",
    "         scoring=f2_scorer,\n",
    "         n_jobs=-1,\n",
    "         verbose=2,\n",
    "         random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_classifier_pipelines_random[clf_name] = random_search.best_estimator_\n",
    "    print(f\"Best parameters for {clf_name}:\")\n",
    "    print(random_search.best_params_)\n",
    "    print(f\"Best CV F2 Score for {clf_name}: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Best Pipeline Details for Each Model =====\n",
      "\n",
      "Model: LogisticRegression\n",
      "Pipeline parameters:\n",
      "  outlier: IsolationForestRemover()\n",
      "  preprocessor__num__transformer: LogTransformSkewed(skewed_cols=['IncomePerYearExp', 'YearsSinceLastPromotion',\n",
      "                                'YearsAtCompany', 'MonthlyIncome',\n",
      "                                'TotalWorkingYears', 'NumCompaniesWorked',\n",
      "                                'DistanceFromHome', 'YearsInCurrentRole',\n",
      "                                'PercentSalaryHike', 'YearsWithCurrManager'])\n",
      "  preprocessor__num__scaler: MinMaxScaler()\n",
      "  preprocessor__bus__encoder: CustomOrdinalEncoder(mapping={'Non-Travel': 0, 'Travel_Frequently': 2,\n",
      "                              'Travel_Rarely': 1})\n",
      "  feat_sel: SelectFromModel(estimator=LogisticRegression(penalty='l1', random_state=42,\n",
      "                                             solver='liblinear'))\n",
      "  classifier: LogisticRegression(C=1, max_iter=500, random_state=42, solver='saga')\n",
      "  Classifier Hyperparameters:\n",
      "    classifier__C: 1\n",
      "    classifier__class_weight: None\n",
      "    classifier__dual: False\n",
      "    classifier__fit_intercept: True\n",
      "    classifier__intercept_scaling: 1\n",
      "    classifier__l1_ratio: None\n",
      "    classifier__max_iter: 500\n",
      "    classifier__multi_class: deprecated\n",
      "    classifier__n_jobs: None\n",
      "    classifier__penalty: l2\n",
      "    classifier__random_state: 42\n",
      "    classifier__solver: saga\n",
      "    classifier__tol: 0.0001\n",
      "    classifier__verbose: 0\n",
      "    classifier__warm_start: False\n",
      "Performance Metrics (CV):\n",
      "  Mean F2 Score  = 0.6220 (Std: 0.0815)\n",
      "  Mean Precision = 0.3859 (Std: 0.0554)\n",
      "  Mean Recall    = 0.7368 (Std: 0.1012)\n",
      "  Mean Accuracy  = 0.7662 (Std: 0.0380)\n",
      "  Mean ROC AUC   = 0.8277 (Std: 0.0543)\n",
      "Test set score (default metric): 0.7823\n",
      "==================================================\n",
      "\n",
      "Model: DecisionTree\n",
      "Pipeline parameters:\n",
      "  outlier: passthrough\n",
      "  preprocessor__num__transformer: LogTransformSkewed(skewed_cols=['IncomePerYearExp', 'YearsSinceLastPromotion',\n",
      "                                'YearsAtCompany', 'MonthlyIncome',\n",
      "                                'TotalWorkingYears', 'NumCompaniesWorked',\n",
      "                                'DistanceFromHome', 'YearsInCurrentRole',\n",
      "                                'PercentSalaryHike', 'YearsWithCurrManager'])\n",
      "  preprocessor__num__scaler: MinMaxScaler()\n",
      "  preprocessor__bus__encoder: CustomOrdinalEncoder(mapping={'Non-Travel': 0, 'Travel_Frequently': 2,\n",
      "                              'Travel_Rarely': 1})\n",
      "  feat_sel: RFE(estimator=LogisticRegression(max_iter=500, random_state=42),\n",
      "    n_features_to_select=10)\n",
      "  classifier: DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_leaf=2,\n",
      "                       min_samples_split=10, random_state=42)\n",
      "  Classifier Hyperparameters:\n",
      "    classifier__ccp_alpha: 0.0\n",
      "    classifier__class_weight: None\n",
      "    classifier__criterion: entropy\n",
      "    classifier__max_depth: 5\n",
      "    classifier__max_features: None\n",
      "    classifier__max_leaf_nodes: None\n",
      "    classifier__min_impurity_decrease: 0.0\n",
      "    classifier__min_samples_leaf: 2\n",
      "    classifier__min_samples_split: 10\n",
      "    classifier__min_weight_fraction_leaf: 0.0\n",
      "    classifier__monotonic_cst: None\n",
      "    classifier__random_state: 42\n",
      "    classifier__splitter: best\n",
      "Performance Metrics (CV):\n",
      "  Mean F2 Score  = 0.4742 (Std: 0.0453)\n",
      "  Mean Precision = 0.4431 (Std: 0.0847)\n",
      "  Mean Recall    = 0.4895 (Std: 0.0591)\n",
      "  Mean Accuracy  = 0.8104 (Std: 0.0329)\n",
      "  Mean ROC AUC   = 0.7103 (Std: 0.0479)\n",
      "Test set score (default metric): 0.7925\n",
      "==================================================\n",
      "\n",
      "Model: RandomForest\n",
      "Pipeline parameters:\n",
      "  outlier: passthrough\n",
      "  preprocessor__num__transformer: LogTransformSkewed(skewed_cols=['IncomePerYearExp', 'YearsSinceLastPromotion',\n",
      "                                'YearsAtCompany', 'MonthlyIncome',\n",
      "                                'TotalWorkingYears', 'NumCompaniesWorked',\n",
      "                                'DistanceFromHome', 'YearsInCurrentRole',\n",
      "                                'PercentSalaryHike', 'YearsWithCurrManager'])\n",
      "  preprocessor__num__scaler: MinMaxScaler()\n",
      "  preprocessor__bus__encoder: CustomOrdinalEncoder(mapping={'Non-Travel': 0, 'Travel_Frequently': 2,\n",
      "                              'Travel_Rarely': 1})\n",
      "  feat_sel: RFE(estimator=LogisticRegression(max_iter=500, random_state=42),\n",
      "    n_features_to_select=10)\n",
      "  classifier: RandomForestClassifier(max_depth=5, min_samples_leaf=2, min_samples_split=5,\n",
      "                       random_state=42)\n",
      "  Classifier Hyperparameters:\n",
      "    classifier__bootstrap: True\n",
      "    classifier__ccp_alpha: 0.0\n",
      "    classifier__class_weight: None\n",
      "    classifier__criterion: gini\n",
      "    classifier__max_depth: 5\n",
      "    classifier__max_features: sqrt\n",
      "    classifier__max_leaf_nodes: None\n",
      "    classifier__max_samples: None\n",
      "    classifier__min_impurity_decrease: 0.0\n",
      "    classifier__min_samples_leaf: 2\n",
      "    classifier__min_samples_split: 5\n",
      "    classifier__min_weight_fraction_leaf: 0.0\n",
      "    classifier__monotonic_cst: None\n",
      "    classifier__n_estimators: 100\n",
      "    classifier__n_jobs: None\n",
      "    classifier__oob_score: False\n",
      "    classifier__random_state: 42\n",
      "    classifier__verbose: 0\n",
      "    classifier__warm_start: False\n",
      "Performance Metrics (CV):\n",
      "  Mean F2 Score  = 0.4807 (Std: 0.0403)\n",
      "  Mean Precision = 0.4581 (Std: 0.0349)\n",
      "  Mean Recall    = 0.4895 (Std: 0.0567)\n",
      "  Mean Accuracy  = 0.8223 (Std: 0.0132)\n",
      "  Mean ROC AUC   = 0.7519 (Std: 0.0459)\n",
      "Test set score (default metric): 0.7721\n",
      "==================================================\n",
      "\n",
      "Model: GradientBoosting\n",
      "Pipeline parameters:\n",
      "  outlier: passthrough\n",
      "  preprocessor__num__transformer: LogTransformSkewed(skewed_cols=['IncomePerYearExp', 'YearsSinceLastPromotion',\n",
      "                                'YearsAtCompany', 'MonthlyIncome',\n",
      "                                'TotalWorkingYears', 'NumCompaniesWorked',\n",
      "                                'DistanceFromHome', 'YearsInCurrentRole',\n",
      "                                'PercentSalaryHike', 'YearsWithCurrManager'])\n",
      "  preprocessor__num__scaler: MinMaxScaler()\n",
      "  preprocessor__bus__encoder: OneHotEncoder(drop='first')\n",
      "  feat_sel: SelectFromModel(estimator=LogisticRegression(penalty='l1', random_state=42,\n",
      "                                             solver='liblinear'))\n",
      "  classifier: GradientBoostingClassifier(learning_rate=0.01, max_features='log2',\n",
      "                           n_estimators=200, random_state=42)\n",
      "  Classifier Hyperparameters:\n",
      "    classifier__ccp_alpha: 0.0\n",
      "    classifier__criterion: friedman_mse\n",
      "    classifier__init: None\n",
      "    classifier__learning_rate: 0.01\n",
      "    classifier__loss: log_loss\n",
      "    classifier__max_depth: 3\n",
      "    classifier__max_features: log2\n",
      "    classifier__max_leaf_nodes: None\n",
      "    classifier__min_impurity_decrease: 0.0\n",
      "    classifier__min_samples_leaf: 1\n",
      "    classifier__min_samples_split: 2\n",
      "    classifier__min_weight_fraction_leaf: 0.0\n",
      "    classifier__n_estimators: 200\n",
      "    classifier__n_iter_no_change: None\n",
      "    classifier__random_state: 42\n",
      "    classifier__subsample: 1.0\n",
      "    classifier__tol: 0.0001\n",
      "    classifier__validation_fraction: 0.1\n",
      "    classifier__verbose: 0\n",
      "    classifier__warm_start: False\n",
      "Performance Metrics (CV):\n",
      "  Mean F2 Score  = 0.4867 (Std: 0.0215)\n",
      "  Mean Precision = 0.5219 (Std: 0.0401)\n",
      "  Mean Recall    = 0.4789 (Std: 0.0197)\n",
      "  Mean Accuracy  = 0.8444 (Std: 0.0112)\n",
      "  Mean ROC AUC   = 0.7978 (Std: 0.0552)\n",
      "Test set score (default metric): 0.8095\n",
      "==================================================\n",
      "\n",
      "Model: MLP\n",
      "Pipeline parameters:\n",
      "  outlier: passthrough\n",
      "  preprocessor__num__transformer: LogTransformSkewed(skewed_cols=['IncomePerYearExp', 'YearsSinceLastPromotion',\n",
      "                                'YearsAtCompany', 'MonthlyIncome',\n",
      "                                'TotalWorkingYears', 'NumCompaniesWorked',\n",
      "                                'DistanceFromHome', 'YearsInCurrentRole',\n",
      "                                'PercentSalaryHike', 'YearsWithCurrManager'])\n",
      "  preprocessor__num__scaler: MinMaxScaler()\n",
      "  preprocessor__bus__encoder: CustomOrdinalEncoder(mapping={'Non-Travel': 0, 'Travel_Frequently': 2,\n",
      "                              'Travel_Rarely': 1})\n",
      "  feat_sel: passthrough\n",
      "  classifier: MLPClassifier(activation='tanh', alpha=0.01, hidden_layer_sizes=(50,),\n",
      "              learning_rate_init=0.01, max_iter=500, random_state=42)\n",
      "  Classifier Hyperparameters:\n",
      "    classifier__activation: tanh\n",
      "    classifier__alpha: 0.01\n",
      "    classifier__batch_size: auto\n",
      "    classifier__beta_1: 0.9\n",
      "    classifier__beta_2: 0.999\n",
      "    classifier__early_stopping: False\n",
      "    classifier__epsilon: 1e-08\n",
      "    classifier__hidden_layer_sizes: (50,)\n",
      "    classifier__learning_rate: constant\n",
      "    classifier__learning_rate_init: 0.01\n",
      "    classifier__max_fun: 15000\n",
      "    classifier__max_iter: 500\n",
      "    classifier__momentum: 0.9\n",
      "    classifier__n_iter_no_change: 10\n",
      "    classifier__nesterovs_momentum: True\n",
      "    classifier__power_t: 0.5\n",
      "    classifier__random_state: 42\n",
      "    classifier__shuffle: True\n",
      "    classifier__solver: adam\n",
      "    classifier__tol: 0.0001\n",
      "    classifier__validation_fraction: 0.1\n",
      "    classifier__verbose: False\n",
      "    classifier__warm_start: False\n",
      "Performance Metrics (CV):\n",
      "  Mean F2 Score  = 0.5543 (Std: 0.0624)\n",
      "  Mean Precision = 0.5234 (Std: 0.0637)\n",
      "  Mean Recall    = 0.5632 (Std: 0.0636)\n",
      "  Mean Accuracy  = 0.8461 (Std: 0.0216)\n",
      "  Mean ROC AUC   = 0.8100 (Std: 0.0370)\n",
      "Test set score (default metric): 0.8265\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_best_pipeline_details(pipeline_dict, X_train, y_train, X_test, y_test, cv=5):\n",
    "    scoring = {\n",
    "        'f2': f2_scorer,\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'accuracy': 'accuracy',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }\n",
    "    print(\"\\n===== Best Pipeline Details for Each Model =====\\n\")\n",
    "    for model_name, pipeline in pipeline_dict.items():\n",
    "        params = pipeline.get_params()\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(\"Pipeline parameters:\")\n",
    "        print(f\"  outlier: {params.get('outlier')}\")\n",
    "        print(f\"  preprocessor__num__transformer: {params.get('preprocessor__num__transformer')}\")\n",
    "        print(f\"  preprocessor__num__scaler: {params.get('preprocessor__num__scaler')}\")\n",
    "        print(f\"  preprocessor__bus__encoder: {params.get('preprocessor__bus__encoder')}\")\n",
    "        print(f\"  feat_sel: {params.get('feat_sel')}\")\n",
    "        print(f\"  classifier: {params.get('classifier')}\")\n",
    "\n",
    "        # Print classifier hyperparams\n",
    "        clf_params = {k: v for k, v in params.items() if k.startswith('classifier__')}\n",
    "        if clf_params:\n",
    "            print(\"  Classifier Hyperparameters:\")\n",
    "            for k, v in clf_params.items():\n",
    "                print(f\"    {k}: {v}\")\n",
    "\n",
    "        cv_results = cross_validate(pipeline, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "        mean_f2 = cv_results['test_f2'].mean()\n",
    "        std_f2 = cv_results['test_f2'].std()\n",
    "        mean_precision = cv_results['test_precision'].mean()\n",
    "        std_precision = cv_results['test_precision'].std()\n",
    "        mean_recall = cv_results['test_recall'].mean()\n",
    "        std_recall = cv_results['test_recall'].std()\n",
    "        mean_accuracy = cv_results['test_accuracy'].mean()\n",
    "        std_accuracy = cv_results['test_accuracy'].std()\n",
    "        mean_roc_auc = cv_results['test_roc_auc'].mean()\n",
    "        std_roc_auc = cv_results['test_roc_auc'].std()\n",
    "\n",
    "        test_score = pipeline.score(X_test, y_test)\n",
    "        print(\"Performance Metrics (CV):\")\n",
    "        print(f\"  Mean F2 Score  = {mean_f2:.4f} (Std: {std_f2:.4f})\")\n",
    "        print(f\"  Mean Precision = {mean_precision:.4f} (Std: {std_precision:.4f})\")\n",
    "        print(f\"  Mean Recall    = {mean_recall:.4f} (Std: {std_recall:.4f})\")\n",
    "        print(f\"  Mean Accuracy  = {mean_accuracy:.4f} (Std: {std_accuracy:.4f})\")\n",
    "        print(f\"  Mean ROC AUC   = {mean_roc_auc:.4f} (Std: {std_roc_auc:.4f})\")\n",
    "        print(f\"Test set score (default metric): {test_score:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "display_best_pipeline_details(best_classifier_pipelines_random, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/Documents/Employee-Attrition/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 10 is smaller than n_iter=20. Running 10 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END final_estimator__C=0.01, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.7s\n",
      "[CV] END final_estimator__C=0.01, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=   9.8s\n",
      "[CV] END final_estimator__C=0.01, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.0s\n",
      "[CV] END final_estimator__C=0.01, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.0s\n",
      "[CV] END final_estimator__C=0.01, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=  10.1s\n",
      "[CV] END final_estimator__C=0.01, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.4s\n",
      "[CV] END final_estimator__C=0.01, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.9s\n",
      "[CV] END final_estimator__C=0.01, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=  10.9s\n",
      "[CV] END final_estimator__C=0.1, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.7s\n",
      "[CV] END final_estimator__C=0.01, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=  11.1s\n",
      "[CV] END final_estimator__C=0.1, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.9s\n",
      "[CV] END final_estimator__C=0.01, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=  11.3s\n",
      "[CV] END final_estimator__C=0.1, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  11.1s\n",
      "[CV] END final_estimator__C=0.1, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=  10.6s\n",
      "[CV] END final_estimator__C=0.1, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  11.6s\n",
      "[CV] END final_estimator__C=0.1, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.8s\n",
      "[CV] END final_estimator__C=0.1, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.6s\n",
      "[CV] END final_estimator__C=0.1, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=  10.0s\n",
      "[CV] END final_estimator__C=0.1, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.6s\n",
      "[CV] END final_estimator__C=1, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=   9.2s\n",
      "[CV] END final_estimator__C=0.1, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.8s\n",
      "[CV] END final_estimator__C=1, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=   9.5s\n",
      "[CV] END final_estimator__C=1, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.0s\n",
      "[CV] END final_estimator__C=1, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.0s\n",
      "[CV] END final_estimator__C=1, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.4s\n",
      "[CV] END final_estimator__C=1, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=   9.5s\n",
      "[CV] END final_estimator__C=1, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.5s\n",
      "[CV] END final_estimator__C=1, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.7s\n",
      "[CV] END final_estimator__C=1, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.6s\n",
      "[CV] END final_estimator__C=10, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=   9.2s\n",
      "[CV] END final_estimator__C=1, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=  10.2s\n",
      "[CV] END final_estimator__C=10, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.2s\n",
      "[CV] END final_estimator__C=10, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=   9.4s\n",
      "[CV] END final_estimator__C=10, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=   9.6s\n",
      "[CV] END final_estimator__C=10, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.4s\n",
      "[CV] END final_estimator__C=10, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.0s\n",
      "[CV] END final_estimator__C=10, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.5s\n",
      "[CV] END final_estimator__C=10, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.9s\n",
      "[CV] END final_estimator__C=10, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=  10.2s\n",
      "[CV] END final_estimator__C=10, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.6s\n",
      "[CV] END final_estimator__C=100, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=   9.5s\n",
      "[CV] END final_estimator__C=100, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=   9.6s\n",
      "[CV] END final_estimator__C=100, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=   9.7s\n",
      "[CV] END final_estimator__C=100, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.2s\n",
      "[CV] END final_estimator__C=100, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.5s\n",
      "[CV] END final_estimator__C=100, final_estimator__penalty=l2, final_estimator__solver=liblinear, passthrough=False; total time=  10.2s\n",
      "[CV] END final_estimator__C=100, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   9.5s\n",
      "[CV] END final_estimator__C=100, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=  10.2s\n",
      "[CV] END final_estimator__C=100, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   7.2s\n",
      "[CV] END final_estimator__C=100, final_estimator__penalty=l2, final_estimator__solver=saga, passthrough=False; total time=   6.8s\n",
      "Best stacking parameters (Logistic Meta-Model):\n",
      "{'passthrough': False, 'final_estimator__solver': 'liblinear', 'final_estimator__penalty': 'l2', 'final_estimator__C': 100}\n",
      "Best stacking CV F2-score: 0.46442381195388405\n",
      "Best stacking CV ROC-AUC: 0.8323765304662981\n",
      "Test ROC-AUC for best stacking ensemble: 0.8110087001464381\n",
      "Test set accuracy for best stacking ensemble: 0.8775510204081632\n"
     ]
    }
   ],
   "source": [
    "estimators = [(name, pipe) for name, pipe in best_classifier_pipelines_random.items()]\n",
    "meta_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "stacking_param_grid = {\n",
    "    'final_estimator__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'final_estimator__penalty': ['l2'],\n",
    "    'final_estimator__solver': ['liblinear', 'saga'],\n",
    "    'passthrough': [False]\n",
    "}\n",
    "\n",
    "scoring_dict = {'f2': f2_scorer, 'roc_auc': 'roc_auc'}\n",
    "n_iter = 20\n",
    "\n",
    "stacking_search_lr = RandomizedSearchCV(\n",
    "    stacking_clf,\n",
    "    param_distributions=stacking_param_grid,\n",
    "    n_iter=n_iter,\n",
    "    cv=5,\n",
    "    scoring=scoring_dict,\n",
    "    refit='f2',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "stacking_search_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best stacking parameters (Logistic Meta-Model):\")\n",
    "print(stacking_search_lr.best_params_)\n",
    "print(\"Best stacking CV F2-score:\", stacking_search_lr.best_score_)\n",
    "\n",
    "best_index = stacking_search_lr.best_index_\n",
    "mean_cv_roc_auc = stacking_search_lr.cv_results_['mean_test_roc_auc'][best_index]\n",
    "print(\"Best stacking CV ROC-AUC:\", mean_cv_roc_auc)\n",
    "\n",
    "y_proba = stacking_search_lr.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "test_roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(\"Test ROC-AUC for best stacking ensemble:\", test_roc_auc)\n",
    "stacking_test_accuracy = stacking_search_lr.best_estimator_.score(X_test, y_test)\n",
    "print(\"Test set accuracy for best stacking ensemble:\", stacking_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LogisticRegression tuned model to models/LogisticRegression_tuned_model.joblib\n",
      "Saved DecisionTree tuned model to models/DecisionTree_tuned_model.joblib\n",
      "Saved RandomForest tuned model to models/RandomForest_tuned_model.joblib\n",
      "Saved GradientBoosting tuned model to models/GradientBoosting_tuned_model.joblib\n",
      "Saved MLP tuned model to models/MLP_tuned_model.joblib\n",
      "Saved stacking logistic regression ensemble to models/stacking_logistic_regression.joblib\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save each tuned base classifier\n",
    "for model_name, model in best_classifier_pipelines_random.items():\n",
    "    filename = os.path.join(\"models\", f\"{model_name}_tuned_model.joblib\")\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"Saved {model_name} tuned model to {filename}\")\n",
    "\n",
    "# Save stacking (logistic meta-model)\n",
    "stacking_lr_filename = os.path.join(\"models\", \"stacking_logistic_regression.joblib\")\n",
    "joblib.dump(stacking_search_lr.best_estimator_, stacking_lr_filename)\n",
    "print(f\"Saved stacking logistic regression ensemble to {stacking_lr_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
